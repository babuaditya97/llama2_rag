# RAG System using LLama2

This GitHub repository houses the code for a Retrieval-Augmented Generation (RAG) system built on top of LLama2. The RAG system leverages the power of LLama2 as the underlying model and incorporates the all-mpnet-base-v2 model for embedding input queries. The system is designed to provide an efficient and effective solution for retrieval and generation tasks.
colab link: https://colab.research.google.com/drive/1Nar8q5yV2mRF-yJKwOuSUlocEe8NrSps?usp=sharing
## Features

- **LLama2 Integration**: The RAG system is built on top of LLama2, utilizing its capabilities for enhanced language understanding and processing.

- **Embedding with all-mpnet-base-v2**: Input queries are embedded using the all-mpnet-base-v2 model, ensuring a high-quality representation for downstream tasks.

- **In-Memory Simple Vector Store**: The RAG system utilizes the VectorStoreIndex in the LLama index to implement an in-memory simple vector store, enhancing the speed and efficiency of retrieval tasks.

- **Hugging Face Integration**: The system leverages Hugging Face for seamless consumption of the LLama2 model, providing an interface for interacting with the RAG system.

-  LLamaIndex is employed to create an overall pipeline for chat over document systems, enabling efficient communication and interaction.

 
